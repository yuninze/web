모두 컬럼 기준이다. 모두 컬럼 시작한다.
컬럼은 intp가 아닌 label로 다룬다.

append pd.DataFrame classes for 
preprocessed features, 
concatenate feature dataframes

def merging_dataframes(df0,df1,index:str):
    return df0.join(df1,on=[f"{index}"],how='left')
for names in features[1:]:
    final_df = merging_dataframes(final_df, names)

const+coef*x
In: 59: pyspark.vectorAssembler

Return a list of the words in the string, using sep as the delimiter string.
If maxsplit is given, at most maxsplit splits are done 
(thus, the list will have at most maxsplit+1 elements). 
If maxsplit is not specified or -1, 
then there is no limit on the number of splits (all possible splits are made).

q=[q for q in enumerate(tgt.split())]

(t-test)(DID, Fixed Effect)
퍼널 분석, 코호트 분석, 클러스터링: 유저군집화

# 00.prerequisiting
# load q as target
# q=df.frame[targetColLabel]
# load w as variable
# w=df.frame.drop(f"{targetColLabel",axis=1)
# 0.preprocessing, classifying features
# by using select_dtypes like sklearn.compose.make_column_selector
# preprocessing would be better than using make_column_selector
# as make_column_selector requires category requisite for tabulars
# from sklearn.compose import ColumnTransformer
# numeric features: (from_numeric) (to_numeric)
# categorical features: sex, occupation, ordinal int (e.g. rank)



df.groupby('qtr').agg({"realgdp": ["mean",  "std"], "unemp": "mean"})



df = pd.read_csv("Fatalities.csv")

# cdr, per-population
df["fatal_rate"] = df["fatal"] /df["pop"] * 10000
df_demean = df.copy()

# per-state beer tax, meaning as per groupby results
df_demean['Mean_beerTax_byState'] = df_demean.groupby('state').beertax.transform(np.mean)

# per-state cdr per state
df_demean['Mean_fatal_rate_byState'] = df_demean.groupby('state').fatal_rate.transform(np.mean)

# subtract row by the mean (shape[0] would be same as used transform)
df_demean["fatal_rate"] = df_demean["fatal_rate"] - df_demean['Mean_fatal_rate_byState']
df_demean["beertax"] = df_demean["beertax"] - df_demean['Mean_beerTax_byState']

model = sm.OLS(df_demean.fatal_rate, df_demean.beertax)
results2 = model.fit()
print(results2.summary())



{"key_"+a+b:"val_"+a+b for a,b in {a+b:a+b for a in "abcd" for b in "0123" if "a"!="b"}}
({"color":gdstrb(),"tag":q,"size":w*mtl} for q,w in n_nouns.most_common(n_noun))
{q:int(w*mtl) for q,w in noun.most_common(n_noun)}



os.environ->dict:environValues

Attention Mechanism, Word Embedding


피처 스케일링
모델이 smooth function이라서 입출력 스케일이 영향 -> 
k-means, nn, rbf, euclidian distance

logical function (step function)은 리절트가 binary
decision tree model -> sequential step function
GBM, RF 등 SPT은 입출력 스케일 binary... binning 필요

숫자 피처의 분포 고려
특정 값이 나타나는 확률
가우시안 분포
예상되는 리절트가 넓은 범위에 걸쳐 나올 거 같으면 가우시안 분포 x
power transform, logarithm 
복합피처

카운트의 boolean binary 아니면 binning, ranging, quantitization

binning
np.random.randint(start stop count)->array:sample
np.floor_divide(sample,10)->10 bins
np.floor(np.log10(array)) 지수 폭 비닝
작은 수 확장, 큰 수 축소
quantiling

예측값이 연속형 수 단순성형회귀
R sqrt로 평가, 높을 수록 좋음, =<1



###
mean 과 std의 동일화. 푸아송 분포.
variance-stabilizing transformation
bayesian method
box-cox가 더 유효함(안좋았는 곳이 더 resoluting됨 대신 좋았던 곳 resoultion 떨어짐)
stats.boxcox(target,lmbda=0) _> 로가리듬 비닝
nd화 param 검출을 해 줌
q,w=stats.boxcox(targetData)
w=lambdaParam
###

sparse data에는 적용하면 안됨 (피처의 피처가 낮아짐)
min-max 스케일링 [0,1] 범위로 비닝
def minmax(x):
	return x-min(x)/max(x)-min(x)
or
preproc.MinMaxScaler(binningTarget:array)
###
standarzation
def standardazation(x):
	return x-mean(x)/sqrt(variance(x))
or
preproc.StandardScaler().fit_transform(binningTarget:array)
###
def euclidian distance
preproc.normalize(binningTarget:array,axis=0)




binningTarget의 수치 분포를 변화한다...
:입력 피처의 정량적 크기가 크게 다를 때 사용한다.
###
GLM interactive feature
multivariables, polynomial feature
preproc.PolynomialFeatures()
train_test_split(featureArray,testsetArray,    )



###
Text Feature Extraction
BoW, B of n-grams
from sklearn.feature_extraction.text import CountVectorizer
CountVectorizer cursor 생성...
bow=CountVectorizer(input="",token_pattern="(?u)\\b\\w+\\b")
trigram=CountVectorizer(input="",token_pattern="(?u)\\b\\w+\\b",ngram_range(3,3))
*(q.fit_transform(content) for q in bow,trigram),



bow.fit(rawdata:array)
trigram.fit(rawdata:array)
q for q in .get_feature_names(), len()







from datetime import datetime
timedelta(days=90)

df.replace("nan",np.nan)
*(df.loc[q,w] for q in df.index for w in df.columns),

conditional listdir보다 scandir이 3배 빠름
scandir->os.DirEntry
os.scandir.close()
os.DirEntry.name
os.DirEntry.path
os.DirEntry.is_dir()
os.DirEntry.is_file()
os.DirEntry.stat().st_size

A=12345
if A>=500:

Working set is a concept in computer science which defines the amount of 
memory that a process requires in a given time interval.

import pandas as pd
iinfo(obj)
finfo(obj)
pd.interval_range()
pd.cut(x=series,bin=rangearray)
quantile(
    q=arrayAlike,
    axis=matrixWise,
    interpolation=
)

pd.to_datetime(
(arg:1-d array,series,dataframe,dict-like),
yearfirst=,
format="%Y/%d/%m/"
infer_datetime_format=False,
origin=parseableDatetime
unit=parseBasisUnit

pd.to_datetime(target)

pd.Timestamp()

#returns containing-subset
where((bool Series/DataFrame, array-like, or callable),other=,axis=)
#inverse boolean of where
mask((bool Series/DataFrame, array-like, or callable),other=,axis=)

pd.DataFrame.from_dict(
    data=dict,
    orient='index')
orient index, keys to rows (index)
oirent columns, keys to columns (columns)
orient: location of keys


select_dtypes(inc=,exc=)


more than 10_000 rows
eval()->boolVec
query()->satisfying rows
parser가 거의 가틍ㅁ

noMoreRegex
int(''.join([q for q in STRING if q.isdigit()]))

["rangeLabelGen {0}-{1}".format(q,q+10) for q in range(0,10000,10)]

ijk
start stop step
[i:j:k]
items = [tuple(func(y) if i == level else y for i, y in enumerate(x)) for x in self]
{"key_"+a+b:"val_"+a+b for a,b in {a+b:a+b for a in "abcd" for b in "0123" if "a"!="b"}.items()}

f_sexage=pd.DataFrame(list(map(getsexage,f.cn.values)))
f['sex']=f_sexage.iloc[:,0].to_numpy()
f['age']=f_sexage.iloc[:,1].to_numpy()
*map(getmd,f.index.get_level_values(0)),

Fixation binning
frame['agerng']=pd.cut(x=frame.age,bins=pd.interval_range(start=0,end=110,periods=10)).to_numpy()
frame.column.quantile(q=x)->np.float64

Entropy binning for workrng, TErng
frame['wrkrng']=pd.cut(x=frame.age,bins=pd.interval_range(start=0,end=110,periods=10)).to_numpy()

ax4=ax4[ax4>0].dropna(axis=0,how='all')
a0[np.isnan(a0[~np.isnan(a0)])]=0

NaN, NaT, null, 0, ''
pd.isna
>>> pd.isnull
<function isna at 0x7fb4c5cefc80>
pd.notna->i.b.
>>> pd.isnull
<function isna at 0x7fb4c5cefc80>
np.isnan->ufunc

np.triu(mat,k:int)
np.tril(mat,k:int)
k-th diagonal, starting from the main diagonal of a matrix A is the list of entries 
A_{i, j} where i=j.
All off-diagonal elements are zero in a diagonal matrix.

cache=[]
with open('namu.json',encoding='utf-8') as jsonfile:
	for q in range(100):
		cache.append(json.loads(jsonfile.readline()))

fileobject
a=concoction('C:/code/concat/ca',10,10)
b=concoction('C:/code/concat/cz',10,10)
c=concoction('C:/code/concat/la',560,70)
d=concoction('C:/code/concat/lz',560,70)
e=concoction('C:/code/concat/nla',560,70)
fs=[a,b,c,d,e]
f=sansibar(fs,pii='C:/code/concat/pii.csv')

zyeon=pd.pivot_table(a,values='money',index=['idx','name','cn','wm'],aggfunc=np.sum)
zyeon.unstack().to_csv('zyeon.csv',encoding='utf-8-sig')

np.iinfo(obj)
np.finfo(obj)
pd.interval_range()
pd.cut(x=series,bin=rangearray)

100Ка(100УЪ)
r"\((\d+.\d+).\)"
@*
^\S+@\S+$

f_sexage=pd.DataFrame(list(map(getsexage,f.cn.values)))
f['sex']=f_sexage.iloc[:,0].to_numpy()
f['age']=f_sexage.iloc[:,1].to_numpy()
*map(getmd,f.index.get_level_values(0)),

Fixation binning
frame['agerng']=pd.cut(x=frame.age,bins=pd.interval_range(start=0,end=110,periods=10)).to_numpy()
frame.column.quantile(q=x)->np.float64

pd.qcut(x=f.work,q=(0,.5,.6,.7,.75,.8,.85,.9,.95,1.))

f['TErng']=pd.cut(x=f.TE,bins=pd.interval_range(start=0,end=f.TE

Entropy binning for workrng, TErng
frame['wrkrng']=pd.cut(x=frame.age,bins=pd.interval_range(start=0,end=110,periods=10)).to_numpy()

half
float16
single
float32
double
float64